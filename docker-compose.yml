services:
  consumer:
    image: consumer:latest
    container_name: consumer
    environment:
      KAFKA_BROKER: kafka:9092
      KAFKA_INGESTION_TOPIC: streaming
      POSTGRES_DB: postgres
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
    depends_on:
      kafka:
        condition: service_healthy

  producer:
    image: producer:latest
    container_name: producer
    depends_on:
      - consumer
    environment:
      KAFKA_BROKER: kafka:9092
      KAFKA_INGESTION_TOPIC: streaming
      KAFKA_LIVE_DATA_TOPIC: live
    ports:
      - "8001:8000"

  backend:
    image: backend:latest
    container_name: backend
    environment:
      POSTGRES_DB: postgres
      POSTGRES_USER: admin
      POSTGRES_PASSWORD: password
      POSTGRES_HOST: postgres
      POSTGRES_PORT: 5432
      KAFKA_BROKER: kafka:9092
      KAFKA_TOPIC: prediction
    ports:
      - "8000:8000"

  frontend:
    image: frontend:latest
    container_name: frontend
    ports:
      - "3000:80"

  spark-master:
    image: spark-server:latest
    container_name: spark-master
    env_file:
      - ./spark-server/.env
    environment:
      - SPARK_MODE=master
      - SPARK_MASTER_HOST=spark-master
      - SPARK_MASTER_PORT=7077
      - SPARK_UI_PORT=8080
      - SPARK_RPC_AUTHENTICATION_ENABLED=false
    ports:
      - "8089:8080" # Spark UI
      - "7077:7077" # Spark Master port
    volumes:
      - ./spark-server:/scripts
      - ./data:/data

  spark-worker:
    image: spark-server:latest
    #    container_name: spark-worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
    env_file:
      - ./spark-server/.env
    depends_on:
      - spark-master
    volumes:
      - ./spark-server:/scripts
      - ./data:/data

  spark-submit:
    image: spark-server:latest
    container_name: spark-submit
    depends_on:
      spark-master:
        condition: service_started
      kafka:
        condition: service_healthy
    ports:
      - "8002:8002"
      - "8003:8003"
      - "8004:8004"
      - "8005:8005"
    environment:
      - SPARK_MASTER=spark://spark-master:7077
      - SPARK_WORKER_CORES=2
      - SPARK_WORKER_MEMORY=4g
      - SPARK_EXECUTOR_MEMORY=3g
      - SPARK_DRIVER_MEMORY=2g
      - SPARK_EXECUTOR_CORES=1
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_LOCAL_DIRS=/tmp/spark-temp
    env_file:
      - ./spark-server/.env
    volumes:
      - ./spark-server:/scripts
      - ./data:/data
    entrypoint:
      [
        "/bin/bash",
        "-c",
        "while true; do echo 'starting script as backend tasks'; sh /scripts/spark_tasks.sh;sleep 3000; done",
      ]

  mlflow:
    image: ghcr.io/mlflow/mlflow:latest
    container_name: mlflow-server
    ports:
      - "5000:5000"
    env_file:
      - .env
    depends_on:
      - postgres
      - minio
    environment:
      - MLFLOW_TRACKING_URI=postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_db:5432/postgres
      - MLFLOW_ARTIFACT_ROOT=s3://mlflow-artifacts/
      - AWS_ACCESS_KEY_ID=${MINIO_ROOT_USER}
      - AWS_SECRET_ACCESS_KEY=${MINIO_ROOT_PASSWORD}
      - MLFLOW_S3_IGNORE_TLS=true
      - MLFLOW_S3_ENDPOINT_URL=http://minio:9000
    command: >
      /bin/bash -c "pip install psycopg2-binary boto3 &&
      mlflow server
      --backend-store-uri postgresql://${POSTGRES_USER}:${POSTGRES_PASSWORD}@postgres_db:5432/postgres
      --default-artifact-root s3://mlflow-artifacts/
      --host 0.0.0.0
      --port 5000"

volumes:
  minio_data:
    driver: local
  postgres_data:
  spark-data:
