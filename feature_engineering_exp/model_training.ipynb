{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e0497516c9e4a12",
   "metadata": {},
   "source": [
    "# Model Training #\n",
    "\n",
    "Based on the comparison results, we choose to use Random Forest model to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T04:42:02.567195Z",
     "start_time": "2025-03-19T04:41:55.079509Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data contains 1613554 records.\n",
      "The labeled data contains 20000 records.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "\n",
    "# spark = SparkSession.builder.appName(\"ModelTraining\").getOrCreate()\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "                    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "                    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "                    .config(\"spark.executor.instances\", \"4\") \\\n",
    "                    .config(\"spark.network.timeout\", \"600s\") \\\n",
    "                    .config(\"spark.executor.heartbeatInterval\", \"60s\") \\\n",
    "                    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "                    .appName(\"ModelTraining\") \\\n",
    "                    .getOrCreate()\n",
    "\n",
    "# Load CSV dataset\n",
    "raw_df = spark.read.csv(\"data/filtered_raw_data.csv\", header=True, inferSchema=True)\n",
    "label_df = spark.read.csv(\"data/label_table.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Compute magnitudes\n",
    "df = raw_df.withColumn(\"accel_mag\", F.sqrt(F.col(\"acceleration_x\")**2 +\n",
    "                                                F.col(\"acceleration_y\")**2 +\n",
    "                                                F.col(\"acceleration_z\")**2)) \\\n",
    "                .withColumn(\"gyro_mag\", F.sqrt(F.col(\"gyro_x\")**2 +\n",
    "                                               F.col(\"gyro_y\")**2 +\n",
    "                                               F.col(\"gyro_z\")**2))\n",
    "\n",
    "print(f\"The raw data contains {df.count()} records.\")\n",
    "\n",
    "# Aggregate per interval\n",
    "aggregated_df = df.groupBy(\"bookingid\").agg(\n",
    "    F.mean(\"speed\").alias(\"avg_speed\"),\n",
    "    F.stddev(\"speed\").alias(\"std_speed\"),\n",
    "    \n",
    "    F.mean(\"accel_mag\").alias(\"avg_accel_mag\"),\n",
    "    F.max(\"accel_mag\").alias(\"max_accel_mag\"),\n",
    "    F.stddev(\"accel_mag\").alias(\"std_accel_mag\"),\n",
    "    \n",
    "    F.mean(\"gyro_mag\").alias(\"avg_gyro_mag\"),\n",
    "    F.stddev(\"gyro_mag\").alias(\"std_gyro_mag\"),\n",
    "    \n",
    "    F.mean(\"acceleration_x\").alias(\"avg_accel_x\"),\n",
    "    F.stddev(\"acceleration_x\").alias(\"std_accel_x\"),\n",
    "    F.max(\"acceleration_x\").alias(\"max_accel_x\"),\n",
    "    \n",
    "    F.mean(\"acceleration_y\").alias(\"avg_accel_y\"),\n",
    "    F.stddev(\"acceleration_y\").alias(\"std_accel_y\"),\n",
    "    F.max(\"acceleration_y\").alias(\"max_accel_y\"),\n",
    "    \n",
    "    F.mean(\"acceleration_z\").alias(\"avg_accel_z\"),\n",
    "    F.stddev(\"acceleration_z\").alias(\"std_accel_z\"),\n",
    "    F.max(\"acceleration_z\").alias(\"max_accel_z\"),\n",
    "    \n",
    "    F.mean(\"gyro_x\").alias(\"avg_gyro_x\"),\n",
    "    F.stddev(\"gyro_x\").alias(\"std_gyro_x\"),\n",
    "    \n",
    "    F.mean(\"gyro_y\").alias(\"avg_gyro_y\"),\n",
    "    F.stddev(\"gyro_y\").alias(\"std_gyro_y\"),\n",
    "    \n",
    "    F.mean(\"gyro_z\").alias(\"avg_gyro_z\"),\n",
    "    F.stddev(\"gyro_z\").alias(\"std_gyro_z\"),\n",
    "    \n",
    "    F.mean(\"accuracy\").alias(\"avg_accuracy\"),\n",
    "    F.stddev(\"accuracy\").alias(\"std_accuracy\"),\n",
    "    \n",
    "    F.max(\"second\").alias(\"second\"),\n",
    ")\n",
    "\n",
    "labeled_df = aggregated_df.join(label_df, \"bookingid\", \"left\")\n",
    "\n",
    "print(f\"The labeled data contains {labeled_df.count()} records.\")\n",
    "\n",
    "df = labeled_df.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "604986c8d70cf50",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T04:42:04.250512Z",
     "start_time": "2025-03-19T04:42:02.603227Z"
    }
   },
   "outputs": [],
   "source": [
    "# Selected features\n",
    "feature_cols = [\n",
    "    \"std_gyro_z\",\n",
    "    \"std_accel_y\",\n",
    "    \"std_accel_z\",\n",
    "    \"max_accel_x\",\n",
    "    \"avg_speed\",\n",
    "    \"std_accel_x\",\n",
    "    \"max_accel_mag\",\n",
    "    \"std_speed\",\n",
    "    \"std_gyro_mag\",\n",
    "    \"std_gyro_x\",\n",
    "    \"max_accel_z\",\n",
    "    \"avg_gyro_mag\",\n",
    "    \"std_accel_mag\",\n",
    "    \"second\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0a1cfccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set count: 16047\n",
      "Test set count: 3953\n"
     ]
    }
   ],
   "source": [
    "# Create vector assembler to transform the features into a vector column\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_cols,\n",
    "    outputCol=\"features\"\n",
    ")\n",
    "\n",
    "# Split the data into training and test sets (80% training, 20% testing)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "print(f\"Training set count: {train_df.count()}\")\n",
    "print(f\"Test set count: {test_df.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bf96cfd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the RandomForestClassifier\n",
    "rf = RandomForestClassifier(\n",
    "    labelCol=\"label\",\n",
    "    featuresCol=\"features\",\n",
    "    numTrees=50,\n",
    "    maxDepth=8,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "# Train the model\n",
    "model = pipeline.fit(train_df)\n",
    "\n",
    "# Make predictions on the test data\n",
    "predictions = model.transform(test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8a9bc2d1a150ab3e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T04:42:05.066308Z",
     "start_time": "2025-03-19T04:42:04.575473Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7773842651151025\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"label\", \n",
    "    predictionCol=\"prediction\", \n",
    "    metricName=\"accuracy\"\n",
    ")\n",
    "accuracy = evaluator.evaluate(predictions)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "856a8497239ffe75",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T04:42:08.383988Z",
     "start_time": "2025-03-19T04:42:05.073341Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "second: 0.42133536055145576\n",
      "avg_speed: 0.09553662462937278\n",
      "std_accel_y: 0.05134347397656492\n",
      "max_accel_mag: 0.050333961732172014\n",
      "std_speed: 0.048158490088526795\n",
      "std_accel_z: 0.045270011983094306\n",
      "avg_gyro_mag: 0.04198473287920141\n",
      "std_accel_mag: 0.04162121780978348\n",
      "max_accel_x: 0.040539157384469636\n",
      "std_gyro_z: 0.04007635607373611\n",
      "std_accel_x: 0.0362881291112121\n",
      "max_accel_z: 0.031196803243405853\n",
      "std_gyro_mag: 0.02966809726775355\n",
      "std_gyro_x: 0.026647583269251155\n"
     ]
    }
   ],
   "source": [
    "# Display feature importances\n",
    "rf_model = model.stages[-1]\n",
    "feature_importance = rf_model.featureImportances\n",
    "feature_importance_list = [(feature, float(importance)) for feature, importance in zip(feature_cols, feature_importance)]\n",
    "sorted_feature_importance = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
    "print(\"Feature Importances:\")\n",
    "for feature, importance in sorted_feature_importance:\n",
    "    print(f\"{feature}: {importance}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
