{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8a77807f92f26ee",
   "metadata": {},
   "source": [
    "# Safety Category: Models\n",
    "\n",
    "In this notebook, several models are tested on the preprocessed training data.\n",
    "\n",
    "**Models:**\n",
    "\n",
    "- Random Forest\n",
    "- Logistic Regression\n",
    "- Support Vector Machine\n",
    "- Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ba10ef922444b11",
   "metadata": {},
   "source": [
    "## Reading the data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fbc121e30a2defb3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:30:21.908333Z",
     "start_time": "2025-03-19T01:30:14.925198Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data contains 1613554 records.\n",
      "The labeled data contains 20000 records.\n",
      "root\n",
      " |-- bookingid: long (nullable = true)\n",
      " |-- avg_speed: double (nullable = false)\n",
      " |-- std_speed: double (nullable = false)\n",
      " |-- avg_accel_mag: double (nullable = false)\n",
      " |-- max_accel_mag: double (nullable = false)\n",
      " |-- std_accel_mag: double (nullable = false)\n",
      " |-- avg_gyro_mag: double (nullable = false)\n",
      " |-- std_gyro_mag: double (nullable = false)\n",
      " |-- avg_accel_x: double (nullable = false)\n",
      " |-- std_accel_x: double (nullable = false)\n",
      " |-- max_accel_x: double (nullable = false)\n",
      " |-- avg_accel_y: double (nullable = false)\n",
      " |-- std_accel_y: double (nullable = false)\n",
      " |-- max_accel_y: double (nullable = false)\n",
      " |-- avg_accel_z: double (nullable = false)\n",
      " |-- std_accel_z: double (nullable = false)\n",
      " |-- max_accel_z: double (nullable = false)\n",
      " |-- avg_gyro_x: double (nullable = false)\n",
      " |-- std_gyro_x: double (nullable = false)\n",
      " |-- avg_gyro_y: double (nullable = false)\n",
      " |-- std_gyro_y: double (nullable = false)\n",
      " |-- avg_gyro_z: double (nullable = false)\n",
      " |-- std_gyro_z: double (nullable = false)\n",
      " |-- avg_accuracy: double (nullable = false)\n",
      " |-- std_accuracy: double (nullable = false)\n",
      " |-- second: double (nullable = false)\n",
      " |-- label: integer (nullable = true)\n",
      "\n",
      "Number of records: 20000\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, LinearSVC, MultilayerPerceptronClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator, BinaryClassificationEvaluator\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder.appName(\"ModelComparison\").getOrCreate()\n",
    "\n",
    "# Load CSV dataset\n",
    "raw_df = spark.read.csv(\"data/filtered_raw_data.csv\", header=True, inferSchema=True)\n",
    "label_df = spark.read.csv(\"data/label_table.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Compute magnitudes\n",
    "df = raw_df.withColumn(\"accel_mag\", F.sqrt(F.col(\"acceleration_x\")**2 +\n",
    "                                                F.col(\"acceleration_y\")**2 +\n",
    "                                                F.col(\"acceleration_z\")**2)) \\\n",
    "                .withColumn(\"gyro_mag\", F.sqrt(F.col(\"gyro_x\")**2 +\n",
    "                                               F.col(\"gyro_y\")**2 +\n",
    "                                               F.col(\"gyro_z\")**2))\n",
    "\n",
    "print(f\"The raw data contains {df.count()} records.\")\n",
    "\n",
    "# Aggregate per interval\n",
    "aggregated_df = df.groupBy(\"bookingid\").agg(\n",
    "    F.mean(\"speed\").alias(\"avg_speed\"),\n",
    "    F.stddev(\"speed\").alias(\"std_speed\"),\n",
    "    \n",
    "    F.mean(\"accel_mag\").alias(\"avg_accel_mag\"),\n",
    "    F.max(\"accel_mag\").alias(\"max_accel_mag\"),\n",
    "    F.stddev(\"accel_mag\").alias(\"std_accel_mag\"),\n",
    "    \n",
    "    F.mean(\"gyro_mag\").alias(\"avg_gyro_mag\"),\n",
    "    F.stddev(\"gyro_mag\").alias(\"std_gyro_mag\"),\n",
    "    \n",
    "    F.mean(\"acceleration_x\").alias(\"avg_accel_x\"),\n",
    "    F.stddev(\"acceleration_x\").alias(\"std_accel_x\"),\n",
    "    F.max(\"acceleration_x\").alias(\"max_accel_x\"),\n",
    "    \n",
    "    F.mean(\"acceleration_y\").alias(\"avg_accel_y\"),\n",
    "    F.stddev(\"acceleration_y\").alias(\"std_accel_y\"),\n",
    "    F.max(\"acceleration_y\").alias(\"max_accel_y\"),\n",
    "    \n",
    "    F.mean(\"acceleration_z\").alias(\"avg_accel_z\"),\n",
    "    F.stddev(\"acceleration_z\").alias(\"std_accel_z\"),\n",
    "    F.max(\"acceleration_z\").alias(\"max_accel_z\"),\n",
    "    \n",
    "    F.mean(\"gyro_x\").alias(\"avg_gyro_x\"),\n",
    "    F.stddev(\"gyro_x\").alias(\"std_gyro_x\"),\n",
    "    \n",
    "    F.mean(\"gyro_y\").alias(\"avg_gyro_y\"),\n",
    "    F.stddev(\"gyro_y\").alias(\"std_gyro_y\"),\n",
    "    \n",
    "    F.mean(\"gyro_z\").alias(\"avg_gyro_z\"),\n",
    "    F.stddev(\"gyro_z\").alias(\"std_gyro_z\"),\n",
    "    \n",
    "    F.mean(\"accuracy\").alias(\"avg_accuracy\"),\n",
    "    F.stddev(\"accuracy\").alias(\"std_accuracy\"),\n",
    "    \n",
    "    F.max(\"second\").alias(\"second\"),\n",
    ")\n",
    "\n",
    "labeled_df = aggregated_df.join(label_df, \"bookingid\", \"left\")\n",
    "\n",
    "print(f\"The labeled data contains {labeled_df.count()} records.\")\n",
    "\n",
    "df = labeled_df.fillna(0.0)\n",
    "\n",
    "# Show data schema\n",
    "df.printSchema()\n",
    "print(f\"Number of records: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e460c46a77815b",
   "metadata": {},
   "source": [
    "## Preprocessing the data ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a3b1ae4de32f3ea2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:30:21.996857Z",
     "start_time": "2025-03-19T01:30:21.925339Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop non-feature columns\n",
    "df = df.drop(\"bookingID\")\n",
    "\n",
    "# Ensure 'label' is integer type\n",
    "df = df.withColumn(\"label\", F.col(\"label\").cast(\"integer\"))\n",
    "\n",
    "feature_cols = [\n",
    "    \"std_gyro_z\",\n",
    "    \"std_accel_y\",\n",
    "    \"std_accel_z\",\n",
    "    \"max_accel_x\",\n",
    "    \"avg_speed\",\n",
    "    \"std_accel_x\",\n",
    "    \"max_accel_mag\",\n",
    "    \"std_speed\",\n",
    "    \"std_gyro_mag\",\n",
    "    \"std_gyro_x\",\n",
    "    \"max_accel_z\",\n",
    "    \"avg_gyro_mag\",\n",
    "    \"std_accel_mag\",\n",
    "    \"second\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4b2a4f2f9212cae9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:30:23.560577Z",
     "start_time": "2025-03-19T01:30:22.335779Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|features                                                                                                                                                                                                                                                                         |label|\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "|[0.312753069501274,1.0242575007151387,1.1160778342632933,2.1074726651670983,2.5963556529736684,2.079176142125469,4.073721417319177,3.6537257316905993,0.403513256677339,0.9237243161322322,-0.13970713790962144,0.5522614599540046,1.543136136655837,1.9570489520126985E-4]      |0    |\n",
      "|[0.3448334887472017,0.8369679985209503,1.6285484846302354,1.2399870026331157,2.3676560843780696,1.3676760263631167,3.8626516902264427,3.5937489893888057,0.29830267042408576,0.7153284704230265,1.0423838545810014,0.36741653072896113,1.3400651651323934,2.717585302176049E-4]  |1    |\n",
      "|[0.2569803142721249,0.9025459221613218,1.4496696129370692,0.7563667674495667,1.375440082609381,1.8943327239395489,3.6097980279704487,2.3427003049111774,0.44942311925216183,0.9205432418300062,0.0030109294448172827,0.5644176949580396,1.0660854911731459,1.6373330341096337E-4]|0    |\n",
      "|[0.51783567444753,1.3952850293853165,1.2482958390330507,1.5282229599881518,2.157026601580117,1.8242296023507703,5.179293991471531,3.773112780413203,0.39622527980042394,0.87964152756679,-0.029298148953251044,0.46990795745854247,2.2563535082350517,2.661069862142679E-4]      |0    |\n",
      "|[0.3263566255555328,1.5373723631743104,2.011375735095601,1.1212068233125996,0.6429109528516356,3.541154334996611,3.4819720044762503,1.5964434208191969,0.5156909226447368,0.6490024740966903,0.7065404701357819,0.32142919178287943,0.5290779187874435,1.2449544075922363E-4]    |1    |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert features into a single feature vector\n",
    "assembler = VectorAssembler(inputCols=feature_cols, outputCol=\"features\")\n",
    "df = assembler.transform(df)\n",
    "\n",
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features\", outputCol=\"scaled_features\", withStd=True, withMean=False)\n",
    "df = scaler.fit(df).transform(df)\n",
    "\n",
    "# Select only the 'scaled_features' and 'label' columns\n",
    "df = df.select(\"scaled_features\", \"label\")\n",
    "df = df.withColumnRenamed(\"scaled_features\", \"features\")\n",
    "\n",
    "# Show sample processed data\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69294912fa08c0a5",
   "metadata": {},
   "source": [
    "## Split Data for Training and Testing ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "74053ce6822400c8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:30:24.448938Z",
     "start_time": "2025-03-19T01:30:23.577020Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data: 16047 rows\n",
      "Test Data: 3953 rows\n"
     ]
    }
   ],
   "source": [
    "# Split data into train (80%) and test (20%)\n",
    "train_df, test_df = df.randomSplit([0.8, 0.2], seed=42)\n",
    "\n",
    "# Show dataset sizes\n",
    "print(f\"Training Data: {train_df.count()} rows\")\n",
    "print(f\"Test Data: {test_df.count()} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73ae673bcb20833c",
   "metadata": {},
   "source": [
    "## Train the Models ##\n",
    "Each model is trained with train_df, and then predictions are made on test_df."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f953141c04017513",
   "metadata": {},
   "source": [
    "**Train Random Forest**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e2c9f90b37298928",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:30:27.640930Z",
     "start_time": "2025-03-19T01:30:24.453945Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=50)\n",
    "rf_model = rf.fit(train_df)\n",
    "rf_preds = rf_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deb2d8103470de61",
   "metadata": {},
   "source": [
    "**Train Logistic Regression**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a2e27d69f3657e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:30:29.872432Z",
     "start_time": "2025-03-19T01:30:27.654333Z"
    }
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\")\n",
    "lr_model = lr.fit(train_df)\n",
    "lr_preds = lr_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590abb59487258e6",
   "metadata": {},
   "source": [
    "**Train Support Vector Machine (SVM)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "76d33dfaadd0d7b3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:30:30.801006Z",
     "start_time": "2025-03-19T01:30:29.881268Z"
    }
   },
   "outputs": [],
   "source": [
    "svm = LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=10)\n",
    "svm_model = svm.fit(train_df)\n",
    "svm_preds = svm_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2d35f2928cfb47",
   "metadata": {},
   "source": [
    "**Train Neural Network**\n",
    "- The input layer size is the **number of features**.\n",
    "- The output layer size is the **number of unique labels**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "459dd10b48e7fe1e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:30:34.506608Z",
     "start_time": "2025-03-19T01:30:30.810648Z"
    }
   },
   "outputs": [],
   "source": [
    "num_features = len(feature_cols)\n",
    "num_classes = df.select(\"label\").distinct().count()\n",
    "\n",
    "nn = MultilayerPerceptronClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    layers=[num_features, 16, 8, num_classes],  # Example: 3 hidden layers\n",
    "    blockSize=128,\n",
    "    maxIter=100\n",
    ")\n",
    "\n",
    "nn_model = nn.fit(train_df)\n",
    "nn_preds = nn_model.transform(test_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7b92a9cd31c99c",
   "metadata": {},
   "source": [
    "## Evaluate Models ##\n",
    "To check which model performs best, we evaluate accuracy and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6e7e87d67bfa0475",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-19T01:30:36.111790Z",
     "start_time": "2025-03-19T01:30:34.518047Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "  Accuracy = 0.7743\n",
      "  F1-score = 0.6991\n",
      "  AUC      = 0.6936\n",
      "\n",
      "Logistic Regression:\n",
      "  Accuracy = 0.7657\n",
      "  F1-score = 0.6854\n",
      "  AUC      = 0.6153\n",
      "\n",
      "SVM:\n",
      "  Accuracy = 0.7536\n",
      "  F1-score = 0.6482\n",
      "  AUC      = N/A (SVM)\n",
      "\n",
      "Neural Network:\n",
      "  Accuracy = 0.7675\n",
      "  F1-score = 0.6839\n",
      "  AUC      = 0.6139\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Initialize evaluators\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"accuracy\")\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"label\", metricName=\"f1\")\n",
    "evaluator_auc = BinaryClassificationEvaluator(labelCol=\"label\", metricName=\"areaUnderROC\")\n",
    "\n",
    "# Store models and predictions\n",
    "models = {\n",
    "    \"Random Forest\": rf_preds,\n",
    "    \"Logistic Regression\": lr_preds,\n",
    "    \"SVM\": svm_preds,\n",
    "    \"Neural Network\": nn_preds\n",
    "}\n",
    "\n",
    "# Compute metrics for each model\n",
    "for name, preds in models.items():\n",
    "    acc = evaluator_acc.evaluate(preds)\n",
    "    f1 = evaluator_f1.evaluate(preds)\n",
    "    \n",
    "    # Handle AUC calculation (skip for SVM)\n",
    "    if name == \"SVM\":\n",
    "        auc_str = \"N/A (SVM)\"\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Accuracy = {acc:.4f}\")\n",
    "        print(f\"  F1-score = {f1:.4f}\")\n",
    "        print(f\"  AUC      = {auc_str}\\n\")\n",
    "    else:\n",
    "        auc = evaluator_auc.evaluate(preds)\n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  Accuracy = {acc:.4f}\")\n",
    "        print(f\"  F1-score = {f1:.4f}\")\n",
    "        print(f\"  AUC      = {auc:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2486226e11c96a59",
   "metadata": {},
   "source": [
    "## Conclusion ##\n",
    "Based on the evaluate results, we choose the **Random Forest** model which performs the best."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
