{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5fd4bb8b",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "using Random Forest for feature importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f1d721",
   "metadata": {},
   "source": [
    "### Load the raw dataset and label dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12044cc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.ml.feature import VectorAssembler, Imputer\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FeatureEngineering\").getOrCreate()\n",
    "\n",
    "# Load your filtered raw data and label table\n",
    "raw_df = spark.read.csv(\"data/filtered_raw_data.csv\", header=True, inferSchema=True)\n",
    "label_df = spark.read.csv(\"data/label_table.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b8115bd",
   "metadata": {},
   "source": [
    "### Feature Engineering Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1a69c17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The raw data contains 1613554 records.\n",
      "The labeled data contains 20000 records.\n",
      "root\n",
      " |-- bookingid: long (nullable = true)\n",
      " |-- avg_speed: double (nullable = false)\n",
      " |-- std_speed: double (nullable = false)\n",
      " |-- avg_accel_mag: double (nullable = false)\n",
      " |-- max_accel_mag: double (nullable = false)\n",
      " |-- std_accel_mag: double (nullable = false)\n",
      " |-- avg_gyro_mag: double (nullable = false)\n",
      " |-- std_gyro_mag: double (nullable = false)\n",
      " |-- avg_accel_x: double (nullable = false)\n",
      " |-- std_accel_x: double (nullable = false)\n",
      " |-- max_accel_x: double (nullable = false)\n",
      " |-- avg_accel_y: double (nullable = false)\n",
      " |-- std_accel_y: double (nullable = false)\n",
      " |-- max_accel_y: double (nullable = false)\n",
      " |-- avg_accel_z: double (nullable = false)\n",
      " |-- std_accel_z: double (nullable = false)\n",
      " |-- max_accel_z: double (nullable = false)\n",
      " |-- avg_gyro_x: double (nullable = false)\n",
      " |-- std_gyro_x: double (nullable = false)\n",
      " |-- avg_gyro_y: double (nullable = false)\n",
      " |-- std_gyro_y: double (nullable = false)\n",
      " |-- avg_gyro_z: double (nullable = false)\n",
      " |-- std_gyro_z: double (nullable = false)\n",
      " |-- avg_accuracy: double (nullable = false)\n",
      " |-- std_accuracy: double (nullable = false)\n",
      " |-- second: double (nullable = false)\n",
      " |-- label: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Compute magnitudes\n",
    "df = raw_df.withColumn(\"accel_mag\", F.sqrt(F.col(\"acceleration_x\")**2 +\n",
    "                                                F.col(\"acceleration_y\")**2 +\n",
    "                                                F.col(\"acceleration_z\")**2)) \\\n",
    "                .withColumn(\"gyro_mag\", F.sqrt(F.col(\"gyro_x\")**2 +\n",
    "                                               F.col(\"gyro_y\")**2 +\n",
    "                                               F.col(\"gyro_z\")**2))\n",
    "\n",
    "print(f\"The raw data contains {df.count()} records.\")\n",
    "# df.printSchema()\n",
    "\n",
    "# Aggregate per interval\n",
    "aggregated_df = df.groupBy(\"bookingid\").agg(\n",
    "    F.mean(\"speed\").alias(\"avg_speed\"),\n",
    "    F.stddev(\"speed\").alias(\"std_speed\"),\n",
    "    \n",
    "    F.mean(\"accel_mag\").alias(\"avg_accel_mag\"),\n",
    "    F.max(\"accel_mag\").alias(\"max_accel_mag\"),\n",
    "    F.stddev(\"accel_mag\").alias(\"std_accel_mag\"),\n",
    "    \n",
    "    F.mean(\"gyro_mag\").alias(\"avg_gyro_mag\"),\n",
    "    F.stddev(\"gyro_mag\").alias(\"std_gyro_mag\"),\n",
    "    \n",
    "    F.mean(\"acceleration_x\").alias(\"avg_accel_x\"),\n",
    "    F.stddev(\"acceleration_x\").alias(\"std_accel_x\"),\n",
    "    F.max(\"acceleration_x\").alias(\"max_accel_x\"),\n",
    "    \n",
    "    F.mean(\"acceleration_y\").alias(\"avg_accel_y\"),\n",
    "    F.stddev(\"acceleration_y\").alias(\"std_accel_y\"),\n",
    "    F.max(\"acceleration_y\").alias(\"max_accel_y\"),\n",
    "    \n",
    "    F.mean(\"acceleration_z\").alias(\"avg_accel_z\"),\n",
    "    F.stddev(\"acceleration_z\").alias(\"std_accel_z\"),\n",
    "    F.max(\"acceleration_z\").alias(\"max_accel_z\"),\n",
    "    \n",
    "    F.mean(\"gyro_x\").alias(\"avg_gyro_x\"),\n",
    "    F.stddev(\"gyro_x\").alias(\"std_gyro_x\"),\n",
    "    \n",
    "    F.mean(\"gyro_y\").alias(\"avg_gyro_y\"),\n",
    "    F.stddev(\"gyro_y\").alias(\"std_gyro_y\"),\n",
    "    \n",
    "    F.mean(\"gyro_z\").alias(\"avg_gyro_z\"),\n",
    "    F.stddev(\"gyro_z\").alias(\"std_gyro_z\"),\n",
    "    \n",
    "    F.mean(\"accuracy\").alias(\"avg_accuracy\"),\n",
    "    F.stddev(\"accuracy\").alias(\"std_accuracy\"),\n",
    "    \n",
    "    F.max(\"second\").alias(\"second\"),\n",
    ")\n",
    "\n",
    "labeled_df = aggregated_df.join(label_df, \"bookingid\", \"left\")\n",
    "\n",
    "print(f\"The labeled data contains {labeled_df.count()} records.\")\n",
    "\n",
    "labeled_df = labeled_df.fillna(0.0)\n",
    "\n",
    "labeled_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d29ccc23",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = labeled_df.drop(\"bookingid\")  # bookingID is just an identifier\n",
    "\n",
    "feature_names = [c for c in df.columns if c != \"label\"]\n",
    "label_col = \"label\"\n",
    "\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=feature_names,\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceea39f4",
   "metadata": {},
   "source": [
    "### Use Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b00de8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      "second: 0.34831958610430097\n",
      "std_gyro_z: 0.06976817186246109\n",
      "std_accel_mag: 0.06543501155937806\n",
      "std_accel_y: 0.061083091111019554\n",
      "std_accel_z: 0.05973005628429935\n",
      "max_accel_x: 0.05402353959920417\n",
      "avg_speed: 0.05068410718992172\n",
      "std_accel_x: 0.048567837480458016\n",
      "max_accel_mag: 0.04542903789330278\n",
      "std_speed: 0.03800659262731864\n",
      "std_gyro_mag: 0.029149941772508327\n",
      "std_gyro_x: 0.020732527656056106\n",
      "max_accel_z: 0.019757705767866982\n",
      "avg_gyro_mag: 0.019645224001540756\n",
      "avg_accel_y: 0.0151691122129525\n",
      "std_accuracy: 0.012650055297149984\n",
      "std_gyro_y: 0.008925761135445527\n",
      "max_accel_y: 0.005619092965213143\n",
      "avg_accuracy: 0.005424631440508121\n",
      "avg_accel_mag: 0.004690669833803138\n",
      "avg_accel_z: 0.004457650062544988\n",
      "avg_accel_x: 0.004171168189571563\n",
      "avg_gyro_y: 0.0036084901478326387\n",
      "avg_gyro_z: 0.0027978299089945887\n",
      "avg_gyro_x: 0.002153107896347039\n",
      "Selected Features after RF: ['second', 'std_gyro_z', 'std_accel_mag', 'std_accel_y', 'std_accel_z', 'max_accel_x', 'avg_speed', 'std_accel_x', 'max_accel_mag', 'std_speed', 'std_gyro_mag', 'std_gyro_x', 'max_accel_z', 'avg_gyro_mag', 'avg_accel_y', 'std_accuracy']\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(labelCol=label_col, featuresCol=\"features\", numTrees=50)\n",
    "\n",
    "pipeline = Pipeline(stages=[assembler, rf])\n",
    "\n",
    "model = pipeline.fit(df)\n",
    "\n",
    "threshold = 0.01 \n",
    "\n",
    "importances = model.stages[-1].featureImportances\n",
    "feature_importance_list = list(zip(feature_names, importances))\n",
    "\n",
    "sorted_importance = sorted(feature_importance_list, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"Feature Importances:\")\n",
    "for feature, score in sorted_importance:\n",
    "    print(f\"{feature}: {score}\")\n",
    "    \n",
    "rf_selected_features = [feature for feature, score in sorted_importance if score > threshold]\n",
    "print(f\"Selected Features after RF: {rf_selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc747c5",
   "metadata": {},
   "source": [
    "### Compare with Lasso (Logistic Regression + L1 Regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f684cd62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All coefficients: [-0.0412745   0.07612243 -0.04248021  0.01767476  0.          0.00780605\n",
      "  0.         -0.02621662  0.          0.08377648  0.          0.05047708\n",
      "  0.          0.          0.13204311  0.01967786  0.          0.07719079\n",
      "  0.          0.          0.          0.43727746  0.          0.\n",
      "  0.        ]\n",
      "Selected Features after Lasso: [('avg_speed', np.float64(-0.04127450472515698)), ('std_speed', np.float64(0.07612242920432162)), ('avg_accel_mag', np.float64(-0.04248021185555937)), ('max_accel_mag', np.float64(0.017674759630666538)), ('avg_gyro_mag', np.float64(0.007806048012756059)), ('avg_accel_x', np.float64(-0.026216619761205626)), ('max_accel_x', np.float64(0.08377648152781744)), ('std_accel_y', np.float64(0.050477082049784766)), ('std_accel_z', np.float64(0.13204310924908175)), ('max_accel_z', np.float64(0.019677855205955653)), ('std_gyro_x', np.float64(0.07719078840485713)), ('std_gyro_z', np.float64(0.4372774622602128))]\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"label\",\n",
    "    elasticNetParam=1.0,  # L1 regularization (Lasso)\n",
    "    regParam=0.01          # Tune this to control regularization strength\n",
    ")\n",
    "\n",
    "df_vector = assembler.transform(df)\n",
    "\n",
    "lr_model = lr.fit(df_vector)\n",
    "coeffs = lr_model.coefficients.toArray()\n",
    "print(\"All coefficients:\", coeffs)\n",
    "\n",
    "lasso_selected_features = []\n",
    "\n",
    "for i, coef in enumerate(coeffs):\n",
    "    if __builtins__.abs(coef) > 1e-4:  # Adjust threshold as needed\n",
    "        lasso_selected_features.append((feature_names[i], coef))\n",
    "        \n",
    "print(f\"Selected Features after Lasso: {lasso_selected_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a46f28",
   "metadata": {},
   "source": [
    "### Use Pearson Correlation as sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0f57a819",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Correlations with Label:\n",
      "std_accel_z: 0.16911324954611875\n",
      "std_accel_y: 0.1598660172875447\n",
      "max_accel_x: 0.15547573054941072\n",
      "std_accel_x: 0.1549506981637733\n",
      "max_accel_mag: 0.14231543649186762\n",
      "std_accel_mag: 0.13819959104593557\n",
      "std_gyro_x: 0.13628694678962552\n",
      "max_accel_z: 0.13581501960136239\n",
      "std_gyro_z: 0.12625625561851947\n",
      "std_gyro_mag: 0.1013182631444176\n",
      "avg_gyro_mag: 0.08200975272605494\n",
      "std_gyro_y: 0.07465966459074358\n",
      "avg_speed: 0.07185067759246141\n",
      "max_accel_y: 0.039036442852618605\n",
      "std_speed: 0.035391407876509864\n",
      "avg_accel_z: 0.033629392085236444\n",
      "avg_gyro_x: 0.015643340157734675\n",
      "avg_accel_mag: 0.01424314720079094\n"
     ]
    }
   ],
   "source": [
    "correlations = {f: __builtins__.abs(df.stat.corr(f, \"label\")) for f in feature_names}\n",
    "sorted_corr = sorted(correlations.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Feature Correlations with Label:\")\n",
    "for feature, corr in sorted_corr:\n",
    "    if corr > threshold:\n",
    "        print(f\"{feature}: {corr}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e780ae18",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5902b808",
   "metadata": {},
   "source": [
    "| Feature       | RF  | Lasso | Pearson | ✅ Final? | Reason                                      |\n",
    "|--------------|-----|-------|---------|----------|---------------------------------------------|\n",
    "| std_gyro_z     | ✔️  | ✔️    | ✔️      | ✔️        | Top feature in all methods (high importance) |\n",
    "| std_accel_y    | ✔️  | ✔️    | ✔️      | ✔️        | Strong in RF, Lasso, and Pearson correlation |\n",
    "| std_accel_z    | ✔️  | ✔️    | ✔️      | ✔️        | Important in all three methods              |\n",
    "| max_accel_x    | ✔️  | ✔️    | ✔️      | ✔️        | High importance in RF and strong correlation |\n",
    "| avg_speed      | ✔️  | ✔️    | ✔️      | ✔️        | Selected by all methods                     |\n",
    "| std_accel_x    | ✔️  | ❌    | ✔️      | ✔️        | Important in RF and Pearson (Lasso missed)  |\n",
    "| max_accel_mag  | ✔️  | ✔️    | ✔️      | ✔️        | Consensus across all methods                |\n",
    "| std_speed      | ✔️  | ✔️    | ❌      | ✔️        | Key in RF and Lasso (Pearson weaker)        |\n",
    "| std_gyro_mag   | ✔️  | ❌    | ✔️      | ✔️        | RF + Pearson agreement                      |\n",
    "| std_gyro_x     | ✔️  | ✔️    | ✔️      | ✔️        | Important in all three                      |\n",
    "| max_accel_z    | ✔️  | ✔️    | ✔️      | ✔️        | Selected by all methods                     |\n",
    "| avg_gyro_mag   | ✔️  | ✔️    | ✔️      | ✔️        | Consensus across methods                    |\n",
    "| std_accel_mag  | ✔️  | ❌    | ✔️      | ✔️        | RF + Pearson (Lasso dropped)                |\n",
    "| second       | ✔️  | ❌    | ❌      | ⚠️ Maybe | Dominant in RF but might represent time     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d758c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_features = [\n",
    "    \"std_gyro_z\",\n",
    "    \"std_accel_y\",\n",
    "    \"std_accel_z\",\n",
    "    \"max_accel_x\",\n",
    "    \"avg_speed\",\n",
    "    \"std_accel_x\",\n",
    "    \"max_accel_mag\",\n",
    "    \"std_speed\",\n",
    "    \"std_gyro_mag\",\n",
    "    \"std_gyro_x\",\n",
    "    \"max_accel_z\",\n",
    "    \"avg_gyro_mag\",\n",
    "    \"std_accel_mag\",\n",
    "    \"second\",\n",
    "]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
